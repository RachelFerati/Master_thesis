---
title: "Beyond Stories: Exploring the Influence of Emotional Indicators on the Subjective Quality Rating of Stories"
subtitle: "Dataset and Stories analysis"
author: "Rachel Ferati"
date: "`r Sys.Date()`"
output: 
  github_document:
      toc: true
---
\newpage

# Initial Dataset

The intital dataset comes from the experiment of Maroussia Nicolet-dit-Félix. In total, 108 transcriptions were analyzed with the scenario EMOTAIX using the software TROPES. The dataset contains the name of each participant, the total number of emotional words and the total number of different emotional categories for each transcription.

### Set the working directory

```{r, message=FALSE, warning=FALSE}
setwd("~/UNINE/4. Master en Sciences cognitives/4. Master Thesis/Stories analysis")
```

### Path to the initial excel file with data from the 108 transcriptions

```{r, message=FALSE, warning=FALSE}
library(readxl)
Stories_analysis <- read_excel("Stories analysis.xlsx")
View(Stories_analysis)
```

---

# Analysis of diversity in emotional categories with the Shannon index

It was expected that people will give a higher rating to a story that has a higher proportion of emotional granularity compared to a story that has a low proportion (H1A).

To calculate the emotional granularity (different emotional categories), we will use the Shannon index, which calculate the diversity of different emotional categories in light with the total number of emotional categories. As such, a transcription that has 80 emotional words and 25 categories will have a higher index than a story with 18 categories for 80 emotional words.

## Calculation of the Shannon index

### Install packages

```{r, message=FALSE, warning=FALSE}
#install.packages("readxl")
#install.packages("vegan")

library(readxl)
library(vegan)
```

### Verification of data

```{r, message=FALSE, warning=FALSE}
data <- read_excel("Stories analysis.xlsx") # Rename for simplification

names(data) # Verification of column names
head(data) # Verification of first lines of the dataframe
```

### Calculation of the Shannon index

```{r, message=FALSE, warning=FALSE}
calculate_shannon_index_from_summary <- function(total_emotion_words,categories_present) {
# If no emotional words or categories, write NA
  if (total_emotion_words == 0 || categories_present == 0) {
    return(NA)
  }
  
# Calculation of the proportion in emotional categories
 proportions <- rep(1 / categories_present, categories_present)
  
# Calculation of Shannon index
shannon_index <- diversity(proportions, index = "shannon")
  
return(shannon_index)
}
```

The Shannon index can be used to analyze and compare the diversity between different categories. It can take into account the number of each words within those categories. For our experiment, the goal was not to compare the proportion between the different categories, but to compare the proportion between transcriptions. Thus, this function calculate the diversity as if every categories count the same amount of words in them.

### Calculation of the function in every data lines

```{r, message=FALSE, warning=FALSE}
data$shannon_index <- mapply(calculate_shannon_index_from_summary, 
                             data$total_emotion_words, 
                             data$categories_present)
print(data)
```

---

# Analysis of Variability in the dataset that contains the Shannon index

To select 9 stories from the experiment, the dataset will be controlled for variability, that is if the data are normally distributed and thus present stories with a high proportion of different emotional categories and stories with a low proportion.
  
### Histogram to check for variability

In this graph, we can have a first view of the potential variability of the dataset.

```{r, message=FALSE, warning=FALSE, fig.width=6, fig.height=4}
hist(data$shannon_index, 
     main = "Variability in the dataset based on the Shannon Index", 
     xlab = "Shannon Index",
     ylab = "Frequency")
```

### Show the mean, median and SD

This process is to see if the mean and the median of our dataset is similar. This is an indication of a normal distribution of the data.

```{r, message=FALSE, warning=FALSE}
summary(data$shannon_index)

mean_shannonindex <- mean(data$shannon_index, na.rm = TRUE)
sd_shannonindex <- sd(data$shannon_index, na.rm = TRUE)

cat("Mean of Shannon index:", mean_shannonindex, "\n")
cat("Standard Deviation of Shannon index:", sd_shannonindex, "\n")
```

### Show the normal distribution

```{r, message=FALSE, warning=FALSE, fig.width=6, fig.height=4}
library("car")
qqPlot(data$shannon_index, 
       main = "Distribution of the dataset",
       ylab = "Shannon index")
```
Based on those observations, it seems that N° 16 and n°18 are outsiders.

### Boxplot to see the outsiders

This plot provides a better view of the outsider and show a normal distribution.

```{r, message=FALSE, warning=FALSE, fig.width=6, fig.height=4}
boxplot(data$shannon_index, mais = "Distribution and outsiders")
```

### Add column ID

```{r, message=FALSE, warning=FALSE}
library(dplyr)

data <- data %>%
  mutate(ID = row_number())
```

### Distribution of the total of emotional words in the dataset

After observing there are outsiders within the dataset, we will have a look at the distribution of the total of emotional words.

```{r, message=FALSE, warning=FALSE}
summary(data$total_emotion_words)
```

```{r, message=FALSE, warning=FALSE, fig.width=6, fig.height=4}
hist(data$total_emotion_words,
     main = "Histogram of the total of emotional words",
     xlab = "Total of emotional words")
```

\newpage

# Filtered dataset based on the total emotional words

To have a more homogeneous dataset and to avoid having stories that differ too much in terms of the total number of emotional words, the dataset is filtered between the first and third quantiles.
  
### Filtered dataset between Quantiles

```{r, message=FALSE, warning=FALSE}
library(dplyr)

Q1 <- quantile(data$total_emotion_words, 0.25)
Q3 <- quantile(data$total_emotion_words, 0.75)

# Filter the data to include only rows between Q1 and Q3
filtered_data <- data[data$total_emotion_words >= Q1 & data$total_emotion_words <= Q3, ]
print(filtered_data)
```

### Distribution of the Filtered dataset

```{r, message=FALSE, warning=FALSE}
summary(filtered_data$shannon_index) # Verification of the distribution of the Shannon index
```

```{r, message=FALSE, warning=FALSE, fig.width=6, fig.height=4}
hist(filtered_data$shannon_index,
     main = "Histogram of the Shannon index in the filtered dataset",
     xlab = "Shannon index")

```

```{r, message=FALSE, warning=FALSE}
summary(filtered_data$categories_present) # Verification of the distribution
```

```{r, message=FALSE, warning=FALSE, fig.width=6, fig.height=4}
hist(filtered_data$categories_present,
     main = "Histogram of Number of different emotional categories in the filtered dataset",
     xlab = "Number of categories") 
```

```{r, message=FALSE, warning=FALSE, fig.width=6, fig.height=4}
library("car")
qqPlot(filtered_data$categories_present,
       main = "Distribution of Number of different emotional categories in the filtered dataset",
       ylab = "Number of categories") # Distribution
```

### Save as an excel file

```{r, message=FALSE, warning=FALSE}
#Install the package
#install.packages("writexl")

# Load the writexl package
library(writexl)

# Define the file path
excel_file_path <- "filtered_data.xlsx"

# Write the filtered_data data frame to an Excel file
write_xlsx(filtered_data, excel_file_path)
```

### Add column ID

```{r, message=FALSE, warning=FALSE}
library(dplyr)

filtered_data <- filtered_data %>%
  mutate(ID = row_number())
```

The filtered dataset counts 54 transcriptions.

\newpage

# Filtered dataset based on total number of words

To have a more homogeneous dataset, and to avoid having stories that differ too much in terms of the total number of words, the data are filtered around the mean. This also allows that participants will have similar stories in terms of the length.

## Words counting

### Automatic words counting from the stories in Filtered data

```{r, message=FALSE, warning=FALSE}
library(readtext)

# Read every document in a file
directory_path <- "C:\\Users\\rache\\OneDrive\\Documents\\UNINE\\4. Master en Sciences cognitives\\4. Master Thesis\\Transcriptions\\Selected transcriptions"
texts <- readtext(paste0(directory_path, "/*.txt"))

# Word counting every text
count_words <- function(text) {
  words <- unlist(strsplit(text, "\\s+"))  # Separation of the text in words
  return(length(words))  # Counting the number of words
}

# Add the number of words to each text
texts$word_count <- sapply(texts$text, count_words)
```

### Add column ID 

```{r, message=FALSE, warning=FALSE}
library(dplyr)

texts <- texts %>%
  mutate(ID = row_number())
```

### Merge word count into filtered_data based on the Identification (ID)

```{r, message=FALSE, warning=FALSE}
filtered_data <- merge(filtered_data, texts[, c("ID", "word_count")], by = "ID", all.x = TRUE)
```

### Save as a excel file the new Filtered data

```{r, message=FALSE, warning=FALSE}
library(writexl)

write_xlsx(filtered_data, "filtered_data_with_word_count.xlsx")
```

---

## Selection of stories around the mean of word count

### Calculation of the mean and standard deviation (SD)

```{r, message=FALSE, warning=FALSE}
mean_word_count <- mean(filtered_data$word_count, na.rm = TRUE)
sd_word_count <- sd(filtered_data$word_count, na.rm = TRUE)
```

### Define around the mean

```{r, message=FALSE, warning=FALSE}
lower_bound <- mean_word_count - sd_word_count
upper_bound <- mean_word_count + sd_word_count
```

### Filtered texts around the mean

```{r, message=FALSE, warning=FALSE}
filtered_data_mean <- subset(filtered_data, word_count >= lower_bound & word_count <= upper_bound)
```

---

## Final dataset

The final dataset is "filtered_data_mean". The initial dataset with 108 transcriptions was filtered based on the total number of emotional words and the total number of words. The final dataset counts 41 transcriptions.

### Distribution of the Shannon index of the final dataset

```{r, message=FALSE, warning=FALSE}
shannonindex_distribution <- filtered_data_mean %>%
  select("shannon_index")

par(mfrow = c(2, 2))
for (col in names(shannonindex_distribution)) {
  hist(shannonindex_distribution[[col]], breaks = 10, main = paste("Histogram of", col), xlab = "Shannon index", col = "lightblue")
}

shapiro_results_shannonindex <- lapply(shannonindex_distribution, shapiro.test)

shapiro_results_shannonindex
```

The final dataset is normally distributed based on the Shannon index.

\newpage

# Establishment of the common story

For the experiment, a common story will be choosen randomly that will be evaluated by the 60 participants.

### Set the seed for reproducibility

```{r, message=FALSE, warning=FALSE}
set.seed(123)
```

### Randomly select one story from the filtered_data_mean dataset

```{r, message=FALSE, warning=FALSE}
common_story <- filtered_data_mean[sample(nrow(filtered_data), 1), ]
```

### Print the selected common story

```{r, message=FALSE, warning=FALSE}
print(common_story)
```

\newpage

# Random assignment of stories for Group A and Group B
  
### Set the seed for reproducibility

```{r, message=FALSE, warning=FALSE}
set.seed(123)
```

### Vector for the 4 stories with high proportion of different emotional categories

```{r, message=FALSE, warning=FALSE}
high_proportion <- c("SAERBE09", "SADJSO02", "BEJEDI08", "MAANAU12")
```

### Vector for the 4 stories with low proportion of different emotional categories

```{r, message=FALSE, warning=FALSE}
low_proportion <- c("VEYVEM11", "CLCHJO01", "ANJETI02", "COHEIG03")
```

### Randomize stories with a high proportion between Group A and Group B

```{r, message=FALSE, warning=FALSE}
high_randomized <- sample(high_proportion)
```

### Randomize stories with a low proportion between Group A and Group B

```{r, message=FALSE, warning=FALSE}
low_randomized <- sample(low_proportion)
```

### Assign stories to Group A and Group B

```{r, message=FALSE, warning=FALSE}
Group_A <- c(high_randomized[1:2], low_randomized[1:2])
Group_B <- c(high_randomized[3:4], low_randomized[3:4])
```

### Show the results

```{r, message=FALSE, warning=FALSE}
cat("Stories for Group A :", Group_A, "\n")
cat("Stories for Group B :", Group_B, "\n")
```



